import torch
from diffusers import StableDiffusionPipeline  # Aseg√∫rate de importar esto

# Verificar si CUDA est√° disponible
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"üñ•Ô∏è Usando dispositivo: {device}")

# Configuraci√≥n del modelo de Stable Diffusion
model_name_sd = "runwayml/stable-diffusion-v1-5"  # Puedes poner tu modelo aqu√≠
HF_TOKEN = "tu_token_de_huggingface"  # Reemplaza con tu token de Hugging Face

# Cargar el modelo de Stable Diffusion
pipe = StableDiffusionPipeline.from_pretrained(
    model_name_sd,
    revision="fp16" if device == "cuda" else None,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32,
    use_auth_token=HF_TOKEN,
)

# Mover el modelo al dispositivo (GPU o CPU)
pipe = pipe.to(device)

# Verificar que el modelo se carg√≥ correctamente en la GPU
if torch.cuda.is_available():
    print("üåü GPU detectada y modelo movido a la GPU.")
else:
    print("üö® No se detect√≥ GPU. El modelo est√° usando la CPU.")

